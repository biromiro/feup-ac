{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Load imports and data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np \n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "%matplotlib inline \n",
            "from sklearn.metrics import accuracy_score, roc_auc_score\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
            "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.naive_bayes import GaussianNB\n",
            "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, RocCurveDisplay, make_scorer\n",
            "from sklearn.model_selection import learning_curve\n",
            "from imblearn.combine import SMOTETomek\n",
            "from imblearn.under_sampling import TomekLinks\n",
            "import datetime\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
            "from lightgbm import LGBMClassifier\n",
            "from xgboost import XGBClassifier"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_transformed = pd.read_csv('output/loan_dev_transformed.csv')\n",
            "competition_transformed = pd.read_csv('output/loan_comp_transformed.csv')\n",
            "data = pd.read_csv('output/loan_dev.csv')\n",
            "competition = pd.read_csv('output/loan_comp.csv')\n",
            "data_transformed"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data.columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "competition"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# These columns will be used as the inputs of the models\n",
            "input_cols = [\n",
            "'loan_date',\n",
            "'loan_duration',\n",
            "'loan_payments',\n",
            "#'account_frequency',\n",
            "'account_district_region',\n",
            "'account_district_no_inhabitants',\n",
            "'account_district_no_municipalities_0_499',\n",
            "'account_district_no_municipalities_500_1999',\n",
            "'account_district_no_municipalities_2000_9999',\n",
            "'account_district_no_municipalities_10000_plus',\n",
            "'account_district_no_cities',\n",
            "'account_district_ratio_urban_inhabitants',\n",
            "'account_district_average_salary',\n",
            "'account_district_unemployment_rate_95',\n",
            "'account_district_unemployment_rate_96',\n",
            "'account_district_no_enterpreneurs_per_1000_inhabitants',\n",
            "'account_district_no_crimes_95',\n",
            "'account_district_no_crimes_96',\n",
            "'owner_card_type',\n",
            "#'owner_district_region',\n",
            "#'owner_district_no_inhabitants',\n",
            "#'owner_district_no_municipalities_0_499',\n",
            "#'owner_district_no_municipalities_500_1999',\n",
            "#'owner_district_no_municipalities_2000_9999',\n",
            "#'owner_district_no_municipalities_10000_plus',\n",
            "#'owner_district_no_cities',\n",
            "#'owner_district_ratio_urban_inhabitants',\n",
            "#'owner_district_average_salary',\n",
            "#'owner_district_unemployment_rate_95',\n",
            "#'owner_district_unemployment_rate_96',\n",
            "#'owner_district_no_enterpreneurs_per_1000_inhabitants',\n",
            "#'owner_district_no_crimes_95',\n",
            "#'owner_district_no_crimes_96',\n",
            "'count_trans_credits',\n",
            "'count_trans_withdrawals',\n",
            "'count_trans_credit_cash',\n",
            "'count_trans_withdrawal_cash',\n",
            "'count_trans_withdrawal_card',\n",
            "'count_trans_collection_other_bank',\n",
            "'count_trans_remittance_other_bank',\n",
            "'count_trans_ksymbol_interest_credited',\n",
            "'count_trans_ksymbol_household',\n",
            "'count_trans_ksymbol_payment_for_statement',\n",
            "'count_trans_ksymbol_insurance_payment',\n",
            "'count_trans_ksymbol_sanction_interest_if_negative_balance',\n",
            "'count_trans_ksymbol_oldage_pension',\n",
            "'last_trans_balance',\n",
            "'mean_trans_balance',\n",
            "'mean_trans_amount_absolute',\n",
            "'mean_trans_amount_credit',\n",
            "'mean_trans_amount_withdrawal',\n",
            "'mean_trans_amount_signed',\n",
            "'owner_male',\n",
            "'owner_age',\n",
            "'account_age_months',\n",
            "'has_disponent',\n",
            "#'owner_profile'\n",
            "]\n",
            "\n",
            "\n",
            "# The output columns are the genres\n",
            "output_cols = 'Predicted'\n",
            "\n",
            "# Averages to calculate for precision, recall, and f1-score\n",
            "averages = [None, \"macro\", \"weighted\", \"micro\", \"samples\"]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.set_option('display.max_columns', None)\n",
            "pd.set_option('display.max_rows', 20)\n",
            "data.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data.isnull().any()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Predictive problem"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.model_selection import learning_curve\n",
            "from imblearn.pipeline import Pipeline\n",
            "\n",
            "def plot_learning_curve(\n",
            "    title,\n",
            "    train_sizes, \n",
            "    train_scores, \n",
            "    test_scores, \n",
            "    fit_times,\n",
            "    score_times,\n",
            "    axes=None,\n",
            "    ylim=None,\n",
            "):\n",
            "    \"\"\"\n",
            "    Generate 3 plots: the test and training learning curve, the training\n",
            "    samples vs fit times curve, the fit times vs score curve.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    estimator : estimator instance\n",
            "        An estimator instance implementing `fit` and `predict` methods which\n",
            "        will be cloned for each validation.\n",
            "\n",
            "    title : str\n",
            "        Title for the chart.\n",
            "\n",
            "    X : array-like of shape (n_samples, n_features)\n",
            "        Training vector, where ``n_samples`` is the number of samples and\n",
            "        ``n_features`` is the number of features.\n",
            "\n",
            "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
            "        Target relative to ``X`` for classification or regression;\n",
            "        None for unsupervised learning.\n",
            "\n",
            "    axes : array-like of shape (3,), default=None\n",
            "        Axes to use for plotting the curves.\n",
            "\n",
            "    ylim : tuple of shape (2,), default=None\n",
            "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
            "\n",
            "    cv : int, cross-validation generator or an iterable, default=None\n",
            "        Determines the cross-validation splitting strategy.\n",
            "        Possible inputs for cv are:\n",
            "\n",
            "          - None, to use the default 5-fold cross-validation,\n",
            "          - integer, to specify the number of folds.\n",
            "          - :term:`CV splitter`,\n",
            "          - An iterable yielding (train, test) splits as arrays of indices.\n",
            "\n",
            "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
            "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
            "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
            "\n",
            "        Refer :ref:`User Guide <cross_validation>` for the various\n",
            "        cross-validators that can be used here.\n",
            "\n",
            "    n_jobs : int or None, default=None\n",
            "        Number of jobs to run in parallel.\n",
            "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            "        for more details.\n",
            "\n",
            "    train_sizes : array-like of shape (n_ticks,)\n",
            "        Relative or absolute numbers of training examples that will be used to\n",
            "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
            "        as a fraction of the maximum size of the training set (that is\n",
            "        determined by the selected validation method), i.e. it has to be within\n",
            "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
            "        sets. Note that for classification the number of samples usually have\n",
            "        to be big enough to contain at least one sample from each class.\n",
            "        (default: np.linspace(0.1, 1.0, 5))\n",
            "    \"\"\"\n",
            "    if axes is None:\n",
            "        fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
            "\n",
            "    axes = axes.reshape(-1)\n",
            "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
            "    fig = fig.delaxes(axes[-1])\n",
            "    \n",
            "    axes[0].set_title(title)\n",
            "    if ylim is not None:\n",
            "        axes[0].set_ylim(*ylim)\n",
            "    axes[0].set_xlabel(\"Training examples\")\n",
            "    axes[0].set_ylabel(\"Score\")\n",
            "\n",
            "    train_scores_mean = np.mean(train_scores, axis=1)\n",
            "    train_scores_std = np.std(train_scores, axis=1)\n",
            "    test_scores_mean = np.mean(test_scores, axis=1)\n",
            "    test_scores_std = np.std(test_scores, axis=1)\n",
            "    fit_times_mean = np.mean(fit_times, axis=1)\n",
            "    fit_times_std = np.std(fit_times, axis=1)\n",
            "    score_times_mean = np.mean(score_times, axis=1)\n",
            "    score_times_std = np.std(score_times, axis=1)\n",
            "\n",
            "    # Plot learning curve\n",
            "    axes[0].grid()\n",
            "    axes[0].fill_between(\n",
            "        train_sizes,\n",
            "        train_scores_mean - train_scores_std,\n",
            "        train_scores_mean + train_scores_std,\n",
            "        alpha=0.1,\n",
            "        color=\"r\",\n",
            "    )\n",
            "    axes[0].fill_between(\n",
            "        train_sizes,\n",
            "        test_scores_mean - test_scores_std,\n",
            "        test_scores_mean + test_scores_std,\n",
            "        alpha=0.1,\n",
            "        color=\"g\",\n",
            "    )\n",
            "    axes[0].plot(\n",
            "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
            "    )\n",
            "    axes[0].plot(\n",
            "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
            "    )\n",
            "    axes[0].legend(loc=\"best\")\n",
            "\n",
            "    # Plot n_samples vs fit_times\n",
            "    axes[1].grid()\n",
            "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
            "    axes[1].fill_between(\n",
            "        train_sizes,\n",
            "        fit_times_mean - fit_times_std,\n",
            "        fit_times_mean + fit_times_std,\n",
            "        alpha=0.1,\n",
            "    )\n",
            "    axes[1].set_xlabel(\"Training examples\")\n",
            "    axes[1].set_ylabel(\"fit_times\")\n",
            "    axes[1].set_title(\"Scalability of the model\")\n",
            "\n",
            "    # Plot fit_time vs score\n",
            "    fit_time_argsort = fit_times_mean.argsort()\n",
            "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
            "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
            "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
            "    axes[2].grid()\n",
            "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
            "    axes[2].fill_between(\n",
            "        fit_time_sorted,\n",
            "        test_scores_mean_sorted - test_scores_std_sorted,\n",
            "        test_scores_mean_sorted + test_scores_std_sorted,\n",
            "        alpha=0.1,\n",
            "    )\n",
            "    axes[2].set_xlabel(\"fit_times\")\n",
            "    axes[2].set_ylabel(\"Score\")\n",
            "    axes[2].set_title(\"Performance of the model\")\n",
            "\n",
            "    # Plot n_samples vs score_times\n",
            "    axes[3].grid()\n",
            "    axes[3].plot(train_sizes, score_times_mean, \"o-\")\n",
            "    axes[3].fill_between(\n",
            "        train_sizes,\n",
            "        score_times_mean - score_times_std,\n",
            "        score_times_mean + score_times_std,\n",
            "        alpha=0.1,\n",
            "    )\n",
            "    axes[3].set_xlabel(\"Training examples\")\n",
            "    axes[3].set_ylabel(\"score_times\")\n",
            "    axes[3].set_title(\"Scalability of the model\")\n",
            "\n",
            "    # Plot score_time vs score\n",
            "    score_time_argsort = score_times_mean.argsort()\n",
            "    score_time_sorted = score_times_mean[score_time_argsort]\n",
            "    test_scores_mean_sorted = test_scores_mean[score_time_argsort]\n",
            "    test_scores_std_sorted = test_scores_std[score_time_argsort]\n",
            "    axes[4].grid()\n",
            "    axes[4].plot(score_time_sorted, test_scores_mean_sorted, \"o-\")\n",
            "    axes[4].fill_between(\n",
            "        score_time_sorted,\n",
            "        test_scores_mean_sorted - test_scores_std_sorted,\n",
            "        test_scores_mean_sorted + test_scores_std_sorted,\n",
            "        alpha=0.1,\n",
            "    )\n",
            "    axes[4].set_xlabel(\"score_times\")\n",
            "    axes[4].set_ylabel(\"Score\")\n",
            "    axes[4].set_title(\"Performance of the model\")\n",
            "\n",
            "    return plt\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "# The following helper functions are for training and evaluating the model\n",
            "\n",
            "def show_confusion_matrix(cms, target_names):\n",
            "    \"\"\"\n",
            "    This helper function plots the confusion matrices calculated when evaluating the model.\n",
            "    \"\"\"\n",
            "    fig, ax = plt.subplots(figsize=(4, 4))\n",
            "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
            "\n",
            "    gnames = [\"True Negative\", \"False Positive\", \"False Negative\", \"True Positive\"]\n",
            "    gcounts = [f\"{v:0.0f}\" for v in cms.flatten()]\n",
            "    gpercentages = [f\"{v:.2%}\" for v in cms.flatten()/np.sum(cms)]\n",
            "    annot = np.asarray([f\"{name}\\n{count}\\n{percentage}\" for name, count, percentage in zip(gnames, gcounts, gpercentages)]).reshape(2, 2)\n",
            "\n",
            "    sns.heatmap(cms, ax=ax, annot=annot, fmt=\"\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
            "    ax.set_ylabel('Actual')\n",
            "    ax.set_xlabel('Predicted')\n",
            "    ax.xaxis.tick_top()\n",
            "    ax.xaxis.set_label_position('top')\n",
            "\n",
            "def evaluate_model(model, testing_inputs, testing_classes, output_cols, sample_weight=None):\n",
            "    \"\"\"\n",
            "    This helper function prints the report and evaluation metrics for the model.\n",
            "    \"\"\"\n",
            "    predictions = model.predict(testing_inputs)\n",
            "    predictions_prob = (model.predict_proba(testing_inputs))[:, 1]\n",
            "\n",
            "    print(\"=\"*70)\n",
            "    print(f\"Evaluation metrics for {model.__class__.__name__}\")\n",
            "    print(\"=\"*70)\n",
            "\n",
            "    score = model.score(testing_inputs, testing_classes) \n",
            "    print(f\"{model.__class__.__name__}'s default score metric: {score}\")\n",
            "\n",
            "\n",
            "    print(\"Classification report\")\n",
            "    print(\n",
            "        classification_report(testing_classes, predictions, sample_weight=sample_weight, digits=4, zero_division=1)\n",
            "    )\n",
            "\n",
            "    accuracy = accuracy_score(testing_classes, predictions, sample_weight=sample_weight)\n",
            "    print(f\"Accuracy: {accuracy:.4f}\")\n",
            "    print(f\"AUC: {roc_auc_score(testing_classes, predictions_prob, sample_weight=sample_weight):.4f}\")\n",
            "\n",
            "    cms = confusion_matrix(testing_classes, predictions, sample_weight=sample_weight)\n",
            "    show_confusion_matrix(cms, ['no', 'yes'])\n",
            "\n",
            "    print(\"=\"*70)\n",
            "\n",
            "def train_and_evaluate(input_cols, output_cols, model, params, scoring, n_iter=None, sample_weight=None, random_state=42, plot_roc=True, transformed = False, oversample=False):\n",
            "    \"\"\"\n",
            "    This function trains the model and prints the evaluation metrics, as well as the confusion matrices, and learning and scalability plots.\n",
            "    \"\"\"\n",
            "    inputs = data[input_cols].values\n",
            "    classes = data[output_cols].values\n",
            "    \n",
            "    if (transformed):\n",
            "        inputs = data_transformed[input_cols].values\n",
            "        classes = data_transformed[output_cols].values\n",
            "    \n",
            "    (training_inputs, testing_inputs, training_classes, testing_classes) = train_test_split(inputs, classes, test_size=0.2, shuffle=False, random_state=random_state)\n",
            "    \n",
            "    pipeline = Pipeline([('model', model)])\n",
            "    \n",
            "    if (oversample):\n",
            "        pipeline = Pipeline([\n",
            "            ('resampler', SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))),\n",
            "            ('model', model)\n",
            "        ])\n",
            "    \n",
            "    scoring = make_scorer(roc_auc_score, needs_proba=True)\n",
            "\n",
            "    if n_iter == None:\n",
            "        clf = GridSearchCV(pipeline, params, n_jobs=-1, scoring=scoring, verbose=0)\n",
            "    else:\n",
            "       clf = RandomizedSearchCV(\n",
            "            pipeline, params, n_iter=n_iter, scoring=scoring,\n",
            "            n_jobs=-1, random_state=random_state, verbose=0)\n",
            "\n",
            "    train_sizes, train_scores, test_scores, fit_times, score_times = learning_curve(\n",
            "        clf, training_inputs, training_classes, return_times=True, cv=5, n_jobs=-1, random_state=random_state)\n",
            "\n",
            "    plot_learning_curve(f\"Learning curves for {model.__class__.__name__}\",\n",
            "        train_sizes, train_scores, test_scores, fit_times, score_times)\n",
            "    \n",
            "    resclf = clf.fit(training_inputs, training_classes)\n",
            "\n",
            "    if plot_roc:\n",
            "        RocCurveDisplay.from_estimator(resclf, testing_inputs, testing_classes)\n",
            "        plt.show()\n",
            "\n",
            "    if isinstance(model, DecisionTreeClassifier):\n",
            "        plot_tree(resclf.best_estimator_['model'], feature_names=input_cols)\n",
            "        plt.savefig(f'output/{model.__class__.__name__}_tree_diagram.svg')\n",
            "    \n",
            "    print(f\"Best params for {model.__class__.__name__}: {clf.best_params_}\")\n",
            "        \n",
            "    evaluate_model(clf, testing_inputs, testing_classes, output_cols, sample_weight=sample_weight)\n",
            "    return resclf.best_estimator_['model']\n",
            "\n",
            "def predict_model(model, transformed = False):\n",
            "    \"\"\"\n",
            "    A more convenient wrapper around train_and_evaluate, albeit less general.\n",
            "    \"\"\"\n",
            "    inputs = competition[input_cols].values\n",
            "    results = competition[['Id', 'Predicted']].copy()\n",
            "    if (transformed):\n",
            "        inputs = competition_transformed[input_cols].values\n",
            "        results = competition_transformed[['Id', 'Predicted']].copy()\n",
            "    \n",
            "    results['Predicted'] = (model.predict_proba(inputs))[:, 1]\n",
            "    modelname = f'{datetime.datetime.now().strftime(\"%Y_%m_%d_T%HH_%MM_%SS\")}_{model.__class__.__name__}_results'\n",
            "    results.to_csv(f'output/predictive/{modelname}.csv', index=False)\n",
            "    return model\n",
            "\n",
            "def train_and_use_model(model, params, scoring='roc_auc', n_iter=None, sample_weight=None, random_state=42, plot_roc=True, transformed = False, oversample=False):\n",
            "    \"\"\"\n",
            "    A more convenient wrapper around train_and_evaluate, albeit less general.\n",
            "    \"\"\"\n",
            "    clf = train_and_evaluate(input_cols, output_cols, model, params, sample_weight=sample_weight, n_iter=n_iter, random_state=random_state, scoring=scoring, plot_roc=plot_roc, oversample=oversample)\n",
            "    return predict_model(clf, transformed = transformed)\n",
            "\n",
            "def use_model(clf, oversample = False, transformed = False):\n",
            "    inputs = data[input_cols].values\n",
            "    classes = data[output_cols].values\n",
            "    \n",
            "    if (transformed):\n",
            "        inputs = data_transformed[input_cols].values\n",
            "        classes = data_transformed[output_cols].values\n",
            "    \n",
            "    if (oversample):\n",
            "        resampler = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
            "        inputs, classes = resampler.fit_resample(inputs, classes)\n",
            "    \n",
            "    clf.fit(inputs, classes)\n",
            "    return predict_model(clf, transformed=transformed)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dt = train_and_use_model(\n",
            "    DecisionTreeClassifier(random_state=42), \n",
            "    {\n",
            "        \"model__criterion\": ['gini', 'entropy'],\n",
            "        'model__splitter': ['best', 'random'],\n",
            "        \"model__max_depth\": range(1, 15),\n",
            "        'model__max_features': range(1, len(input_cols)),\n",
            "        \"model__min_samples_split\": range(2,15),\n",
            "        \"model__min_samples_leaf\": range(1,7)\n",
            "    },\n",
            "    n_iter=50,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rf = train_and_use_model(\n",
            "    RandomForestClassifier(), \n",
            "    { \n",
            "        'model__n_estimators': [100, 150],\n",
            "        'model__max_features': ['sqrt', 'log2'],\n",
            "        'model__max_depth' : [8,9,10,11,12],\n",
            "        'model__criterion' :['gini', 'entropy'],\n",
            "        'model__bootstrap': [True],\n",
            "        'model__min_samples_leaf': [1,2,3],\n",
            "        'model__min_samples_split': [2,3],\n",
            "    },\n",
            "    n_iter=50\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "gnb = train_and_use_model(\n",
            "    GaussianNB(), \n",
            "    {\n",
            "        'model__var_smoothing': np.logspace(0,-9, num=100)\n",
            "    },\n",
            "    n_iter=50\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "lg = train_and_use_model(\n",
            "    LogisticRegression(),\n",
            "    {\n",
            "        'model__penalty': ['l2'],\n",
            "        'model__C': [100, 10, 1.0, 0.1, 0.01],\n",
            "        'model__solver': ['liblinear']\n",
            "    },\n",
            "    n_iter=50\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "gb = train_and_use_model(\n",
            "    GradientBoostingClassifier(),\n",
            "    {\n",
            "        'model__max_features': range(7,20,2),\n",
            "        'model__min_samples_split':range(1000,2201,400),\n",
            "        'model__min_samples_leaf':range(30,71,10),\n",
            "        'model__max_depth':range(5,16,2),\n",
            "        'model__min_samples_split':range(200,1001,200)\n",
            "    },\n",
            "    n_iter=50\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ada = train_and_use_model(\n",
            "    AdaBoostClassifier(),\n",
            "    {\n",
            "        'model__n_estimators': [10,50,100,500],\n",
            "        'model__learning_rate':[0.0001, 0.001, 0.01, 0.1, 1.0]\n",
            "    },\n",
            "    n_iter=50\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "xgb = train_and_use_model(\n",
            "    XGBClassifier(use_label_encoder=False, eval_metric='aucpr'),\n",
            "    {\n",
            "        'model__eta': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1],\n",
            "        'model__gamma': [0, 1, 5, 10, 100, 1000],\n",
            "        'model__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
            "        'model__min_child_weight': [1, 3, 5, 7],\n",
            "        'model__max_delta_step': [0, 0.2, 0.6, 1, 2],\n",
            "        'model__subsample': [0.6, 0.7, 0.8, 0.9, 1],\n",
            "        'model__sampling_method': ['uniform', 'gradient_based'],\n",
            "        'model__scale_pos_weight': [1, 3, 5, 7, 9],\n",
            "        'model__max_bin': [16, 32, 64, 128, 256, 512],\n",
            "    },\n",
            "    n_iter=50\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "lgb = train_and_use_model(\n",
            "    LGBMClassifier(),\n",
            "    {\n",
            "        'model__num_leaves': [31, 63, 127, 255, 511, 1023],\n",
            "        'model__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
            "        'model__learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1],\n",
            "        'model__n_estimators': [10, 50, 100, 500, 1000, 2000],\n",
            "        'model__subsample_for_bin': [200000, 300000, 400000, 500000],\n",
            "        'model__min_child_samples': [20, 30, 40, 50],\n",
            "    },\n",
            "    n_iter=50\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "level0 = list()\n",
            "level0.append(('lr', LogisticRegression(random_state=42) ))\n",
            "level0.append(('rf', DecisionTreeClassifier(random_state=42)))\n",
            "level0.append(('xgb', XGBClassifier(random_state=42)))\n",
            "level0.append(('lgbm', LGBMClassifier(random_state=42)))\n",
            "\n",
            "level1 = RandomForestClassifier(n_estimators=100, min_samples_split=3, min_samples_leaf=2, max_features='sqrt', max_depth=9, bootstrap=True, criterion='entropy', random_state=42)\n",
            "clf = StackingClassifier(estimators=level0, final_estimator=level1, cv=4)\n",
            "\n",
            "train_and_use_model(clf, {})"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "estimators = [\n",
            "    ('adb', AdaBoostClassifier(random_state=42)),\n",
            "    ('rf', RandomForestClassifier(n_estimators=100, min_samples_split=3, min_samples_leaf=2, max_features='sqrt', max_depth=9, bootstrap=True, criterion='entropy', random_state=42)),\n",
            "]\n",
            "clf = StackingClassifier(\n",
            "    estimators=estimators, final_estimator=LogisticRegression()\n",
            ")\n",
            "\n",
            "train_and_use_model(clf, {})"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.10.7 64-bit",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.7"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
